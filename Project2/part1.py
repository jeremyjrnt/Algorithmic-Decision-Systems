# -*- coding: utf-8 -*-
"""hw2_mesrar_part1-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DuAViP7-lj-tRFOv1LErYrjsLP8NHihe
"""

import csv
import pandas as pd
import numpy as np

test_file_path = '/content/test.csv'
user_clip_file_path = '/content/user_clip.csv'

test_df = pd.read_csv(test_file_path)
user_clip_df = pd.read_csv(user_clip_file_path)

"""**Understanding the data**

PART 1
"""

import matplotlib.pyplot as plt

# Plot the weight distribution
plt.figure(figsize=(10, 6))
plt.hist(user_clip_df['weight'], bins=10000, alpha=0.75, color='blue')
plt.title('Distribution of Viewing Time (Weight)')
plt.xlabel('Weight (seconds)')
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.8)
plt.xlim(1, 7000)

plt.show()

# Weight statistics

statistics = user_clip_df['weight'].describe()

print(statistics)

# Display the top 20 highest values in the 'weight' column in descending order
top_20_weights = user_clip_df['weight'].sort_values(ascending=False).head(20)
print(top_20_weights)

# Calcul des percentiles pour chaque percentile (0 à 100)
percentiles = np.percentile(user_clip_df['weight'], np.arange(0, 101, 1))

# Création d'un DataFrame pour afficher les résultats
percentile_df = pd.DataFrame({
    'Percentile': np.arange(0, 101, 1),
    'Weight': percentiles
})

print(percentile_df)

# Compter le nombre de lignes où 'weight' dépasse 14 400
count_exceeding_14400 = (user_clip_df['weight'] > 14400).sum()

print(f"Nombre de lignes avec 'weight' supérieur à 4 heures : {count_exceeding_14400}")

count_exceeding_32400 = (user_clip_df['weight'] > 32400).sum()

print(f"Nombre de lignes avec 'weight' supérieur à 9 heures : {count_exceeding_32400}")

"""**PART 1.1**"""

r_avg = user_clip_df['weight'].mean()

import numpy as np

user_ids = user_clip_df['user_id'].unique()
clip_ids = user_clip_df['clip_id'].unique()

b_u = {user_id: 10 for user_id in user_ids}
b_i = {clip_id: 10 for clip_id in clip_ids}

def compute_f1_and_gradients(df, r_avg, b_u, b_i, lambda_reg=0.1):
    error = 0
    gradient_bu = {user_id: 0 for user_id in b_u}
    gradient_bi = {clip_id: 0 for clip_id in b_i}

    for _, row in df.iterrows():
        u, i, r_ui = row['user_id'], row['clip_id'], row['weight']
        pred = r_avg + b_u[u] + b_i[i]
        err = r_ui - pred

        error += err**2
        gradient_bu[u] += -2 * err
        gradient_bi[i] += -2 * err

    reg_bu = lambda_reg * sum(bu**2 for bu in b_u.values())
    reg_bi = lambda_reg * sum(bi**2 for bi in b_i.values())

    error += reg_bu + reg_bi

    for u in gradient_bu:
        gradient_bu[u] += 2 * lambda_reg * b_u[u]

    for i in gradient_bi:
        gradient_bi[i] += 2 * lambda_reg * b_i[i]

    return error, gradient_bu, gradient_bi

learning_rate = 1.75e-3
num_iterations = 1000

for it in range(num_iterations):
    error, grad_bu, grad_bi = compute_f1_and_gradients(user_clip_df, r_avg, b_u, b_i)

    for u in b_u:
        b_u[u] -= learning_rate * grad_bu[u]

    for i in b_i:
        b_i[i] -= learning_rate * grad_bi[i]

    if it % 10 == 0 or it == 999:
        print(f'Iteration {it}, Error: {error}')


clip_bias = "b_i_bias.csv"

user_bias = "b_u_bias.csv"

# Écrire dans le fichier CSV
with open(clip_bias, mode='w', newline='') as file:
    writer = csv.writer(file)
    # Écrire les en-têtes
    writer.writerow(["clip_id", "value"])
    # Écrire les données
    for clip_id, value in b_i.items():
        writer.writerow([clip_id, value])

# Écrire dans le fichier CSV
with open(user_bias, mode='w', newline='') as file:
    writer = csv.writer(file)
    # Écrire les en-têtes
    writer.writerow(["user_id", "value"])
    # Écrire les données
    for user_id, value in b_u.items():
        writer.writerow([user_id, value])

predictions = []

# Prédire les valeurs pour chaque paire utilisateur-clip du fichier de test
for _, row in test_df.iterrows():
    user_id = row['user_id']
    clip_id = row['clip_id']
    if user_id in b_u and clip_id in b_i:
        predicted_r_ui = r_avg + b_u[user_id] + b_i[clip_id]
    else:
        predicted_r_ui = 0  # Si l'utilisateur ou le clip est inconnu, utilisez la moyenne générale

    # Ajouter la prédiction à la liste
    predictions.append({
        'id_user': user_id,
        'id_clip': clip_id,
        'weight_pred': max(predicted_r_ui, 0)  # Remplacer les prédictions négatives par 0
    })

# Convertir la liste de prédictions en DataFrame
predictions_df = pd.DataFrame(predictions)

# Sauvegarder les prédictions dans un nouveau fichier CSV
predictions_df.to_csv('test_predictions_with_bias.csv', index=False)

"""**PART 1.2**"""

# Utilisation de la méthode pivot pour créer la matrice
mat = user_clip_df.pivot(index='user_id', columns='clip_id', values='weight')

# Remplir les NaN avec 0 pour les clips non vus
mat = mat.fillna(0)

# Conversion en int si les poids sont des entiers
mat = mat.astype(int)

print(type(mat))
print(mat.head())

U, sigma, Vt = np.linalg.svd(mat, full_matrices=False)

# Approximation de faible rang avec k = 20
k = 20
U_k = U[:, :k]
sigma_k = np.diag(sigma[:k])
Vt_k = Vt[:k, :]

# Reconstruire la matrice approximative
approx_matrix = np.dot(U_k, np.dot(sigma_k, Vt_k))

# Conversion en DataFrame pour une visualisation plus facile
approx_df = pd.DataFrame(approx_matrix, index=mat.index, columns=mat.columns)

f2 = 0
for row in user_clip_df.itertuples():
    # Obtenir les indices des utilisateurs et des films
    user_id = row.user_id
    clip_id = row.clip_id
    # Valeur réelle
    actual_r_ui = row.weight
    # Valeur prédite
    if user_id in approx_df.index and clip_id in approx_df.columns:
         predicted_r_ui = approx_df.loc[user_id, clip_id]
    #else:
    #   predicted_r_ui = 0  # S'ils n'existent pas, prédisez 0

    # Ajouter la différence au carré à f2
    f2 += (predicted_r_ui - actual_r_ui) ** 2
    print(f"Valeur de f2: {f2}")

predictions = []

# Prédire les valeurs pour chaque paire utilisateur-clip du fichier de test
for row in test_df.itertuples():
    user_id = row.user_id
    clip_id = row.clip_id
    if user_id in approx_df.index and clip_id in approx_df.columns:
        predicted_r_ui = approx_df.loc[user_id, clip_id]
    else:
        predicted_r_ui = 0  # Si l'utilisateur ou le clip est inconnu, prédisez 0

    # Ajouter la prédiction à la liste
    predictions.append({
        'id_user': user_id,
        'id_clip': clip_id,
        'weight_pred': max(predicted_r_ui, 0)  # Remplacer les prédictions négatives par 0
    })

# Convertir la liste de prédictions en DataFrame
predictions_df = pd.DataFrame(predictions)

# Sauvegarder les prédictions dans un nouveau fichier CSV
predictions_df.to_csv('test_predictions_svd.csv', index=False)